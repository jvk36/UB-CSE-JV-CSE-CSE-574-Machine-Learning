{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 1 - Understanding upvotes on top reddit posts\n",
    "\n",
    "After completing this project, you will be able to do the following:\n",
    "\n",
    "- Collect and save reddit data using the reddit API (through the ```praw``` library)\n",
    "- Be able to conduct descriptive analyses, via manipulation of data stored in a pandas dataframe, and via the creation and exploration of graphs, of the number of upvotes of reddit comments\n",
    "- Be able to conduct a linear regression to help understand the factors associated with a top post having many upvotes on reddit\n",
    "- **574 Only**: Be able to compare a linear regression model to a Poisson regression model\n",
    "\n",
    "# Resources you can use to complete this assignment (a COMPLETE list)\n",
    "\n",
    "**NOTE: You ARE allowed to use Google to find things that fit this list (i.e. it is often easy to google something like \"plotly draw line graph\" to find the right part of the plotly documentation).**\n",
    "\n",
    "- Anything linked to in this article\n",
    "- Anything linked to from the course web page\n",
    "- Any materials from another online course taught at a university (**if you use this, you MUST provide a link to the exact document used**)\n",
    "- Anything posted by Kenny, Navid, or Yincheng on Piazza\n",
    "\n",
    "# Setup\n",
    "\n",
    "- For this assignment, you will need to install the PRAW library for scraping reddit data.\n",
    "\n",
    "# Grading\n",
    "\n",
    "There are three parts to the grading:\n",
    "\n",
    "1. **Written Report (60 points)**: You will submit a PDF report that provides answers to questions here, and that contains plots we request.  These same questions are also posted in the assignment PDF, for convenience. **Again, the questions in the Assignment PDF and here are the same (for the written report), we just put them in both places for convenience.**.\n",
    "\n",
    "2. **Saved file from Part 1.3 (10 points)**: See below for details. \n",
    "\n",
    "3. **Coding spot checks (30 points)** - We will select 6 problems to spot check. This means that we will check to make sure that your code is written in a reasonable way and that it obtains the desired results when we run your code. For example, your code should not be written in a way that makes it exceedingly slow, e.g. by using for loops where a vectorized approach would be applicable. We will *not* tell you which problems we are spot checking.\n",
    "\n",
    "\n",
    "As such, you will submit, one member of your group will subit as a zip file on UBLearns, a ```.zip``` file that contains 3 things:\n",
    "- Your completed jupyter notebook.\n",
    "- Your written report, answering all questions asked here (and copied in the assignment PDF)\n",
    "- Your saved file from Part 1.3 below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a comment in the code. All comments in python are preceeded by a pound sign\n",
    "# Comments can be plain English, because the computer ignores them when running the code.\n",
    "\n",
    "# This should be all the imports you need for this project.\n",
    "\n",
    "# The line of code below this comment imports code written by other people in the form of the \n",
    "# praw library\n",
    "import praw\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creating a reddit account\n",
    "\n",
    "If you don't have one already, the first thing you'll need to do is go to [reddit](http://www.reddit.com/) and create a reddit account.\n",
    "\n",
    "## Step 2: Creating a reddit app\n",
    "\n",
    "Now, we're going to create a reddit app. Make sure you're signed in to your reddit account, and then go to the [app page](https://ssl.reddit.com/prefs/apps/).  From here, click on the \"create an app\" button. <b>Make sure that you've selected the \"script\" option in the checkbox</b>, and then fill in a name and description. For the two URL fields, it doesn't really matter what you put. Your input should look something like mine.\n",
    "\n",
    "<img width=\"1000px\" src=\"./reddit_tut_0.png\" style=\"max-width:95%;border:3px solid black;\"></img>\n",
    "    \n",
    "Cool! Now hit \"create app\" button.\n",
    "\n",
    "## Step 3: Scraping some reddit data\n",
    "\n",
    "### 3a - setting up your API credentials\n",
    "We're now going to pull down some real, live reddit posts and take a look at them! To do so, we're going to need some information from our app.  Reddit uses this information to keep track of who is accessing their data, and in what ways they're accessing it.  In other words, reddit wants to make sure it knows who might be doing bad things with their data or to their platform, so they can shut those apps down. We'll return to that point in a bit.  But, for now, you'll need to edit the code below to enter in your ```client_id```, your ```client_secret```, and your ```username```  in order for you to be able to collect data.  ***Note - PLEASE TRY TO REMEMBER TO REMOVE YOUR SECRET INFORMATION BEFORE SUBMITTING THE ASSIGNMENT***. You can find the first two on the app creation page, where the red boxes are in the image below:\n",
    "\n",
    "<img width=\"1000px\" src=\"reddit_tut_1.png\" style=\"max-width:95%;border:3px solid black;\"></img>\n",
    "\n",
    "Enter your information between the appropriate quotation marks in the (python) code below, replacing the phrase ```ENTER_YOUR_XXX_HERE```.\n",
    "\n",
    " ***Make sure to push ```SHIFT+ENTER``` after you've changed the code!*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "your_client_id = 'FBqfsrdgbU3Mpw'\n",
    "your_client_secret = 'uDsgEmD6TWNXXGVTgdG8ODjUFhw'\n",
    "your_username = \"ENTER_YOUR_USERNAME_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b - Selecting subreddits\n",
    "\n",
    "Ok, we're almost all set up to scrape! to do so, we're going to use a python library called [praw](https://praw.readthedocs.io/).  Praw is a relatively powerful tool, allowing you to do a bunch of cool things with the reddit API.  For this assignment, you're just going to do the basics:\n",
    "\n",
    "Specifically, **<span style=\"color: red;\">create a variable called ```subreddits``` in the code box below. The variable should point to a ```list``` data structure that has the names of 3 subreddits you want to pull data from.</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "\n",
    "subreddits = ['Jokes',\n",
    "'news', 'science', 'WritingPrompts', 'ShowerThoughts', 'worldnews', \n",
    "               'todayilearned', 'learnprogramming', 'announcements', \n",
    "               'funny', 'food', 'sports', 'gadgets', 'aww', 'mildlyinteresting',\n",
    "               'memes', 'technology', 'travel', 'books',  'gaming', \n",
    "               'cats', 'conspiracy', 'PoliticalHumor', 'hockey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c - Setting up our authentication mechanism for our application\n",
    "\n",
    "The last step before we start crawling is to set ourselves up to make authenticated calls to the reddit API. **<span style=\"color: red;\">Use the PRAW library to create an instance of the class ```praw.Reddit``` that you can use to scrape the reddit API.</span>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "\n",
    "# This line of code calls a function in the praw library.\n",
    "# That function (or block of code, which is hidden in the praw library's code)\n",
    "# sets us up to easily make calls to the reddit API moving forward, without\n",
    "# having to enter this information every time we make a call\n",
    "reddit = praw.Reddit(client_id=your_client_id,\n",
    "                     client_secret=your_client_secret,\n",
    "                     user_agent=your_username + \": my first scraper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d - Finally, some scraping!\n",
    "\n",
    "OK! Now we can finally pull some data down from the reddit API!\n",
    "\n",
    "<span style=\"color: red;\">Use the ```praw``` library to pull down the **top 1000 posts of all time from EACH of the 3 subreddits you selected.** Note: You may not get all 1000, due to oddities with the reddit API. However, your code should specify that it *wants* up to 1000 posts.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: REPLACE\n",
    "\n",
    "# create a list of submissions and load using a progress bar\n",
    "submissions = []\n",
    "n_submissions_to_get = 1000\n",
    "\n",
    "\n",
    "for sr in subreddits:\n",
    "    subreddit = reddit.subreddit(sr)\n",
    "\n",
    "    print(\"You're pulling down posts from: \",subreddit.display_name)\n",
    "\n",
    "    # populate that list \n",
    "    for submission in subreddit.top(time_filter=\"all\",limit=n_submissions_to_get):\n",
    "        submissions.append(submission)\n",
    "\n",
    "    print(len(submissions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering some questions about your data and the API\n",
    "\n",
    "### 1.1 Understanding APIs\n",
    "***Note, Part 1 questions can be answered by carefully reading the [documentation of the PRAW library carefully](https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html) and/or the [reddit API documentation](https://github.com/reddit-archive/reddit/wiki/API#rules).***\n",
    "\n",
    "- **1.1.1** How many API calls were required to collect the submissions?\n",
    "- **1.1.2** Why did we set the submission limit at 1000?\n",
    "- **1.1.3** How long, in minutes, would it take you to collect 1000 posts from 25 different subreddits? What about from 500 different subreddits? *Hint: You'll have to consider how many API requests you are allowed to make in a given time period.*\n",
    "\n",
    "### 1.2 Thinking about your sample\n",
    "\n",
    "You collected (approximately) the top 1000 submissions from 3 different subreddits. \n",
    "\n",
    "- **1.2.1** Do you think these posts are representative of **all** the posts on that subreddit? (Yes or no, only) \n",
    "- **1.2.2** Why or why not? That is, if you think so, why do you think there's not much sampling bias here? If not, what do you think might be different about these top posts than other posts?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving out your data\n",
    "\n",
    "Finally, we're going to save your data out and submit it. For this part, [this section of the API documentation may be useful](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html#determine-available-attributes-of-an-object). Similarly, note that saving CSVs is sometimes easiest by first converting your data into a ```pandas``` dataframe, and then just calling ```.to_csv()```.\n",
    "\n",
    "<span style=\"color: red;\">You will save the data you have collected out to a CSV file. This CSV file should be called ```part1_data.csv```. The file should contain a column for each of the fields listed in the ```fields_to_capture``` list below. Additionally, you should save out the **author's name** (hint, the author attribute of the ```praw.Submission``` data structure is a ```praw.Redditor``` object. You will need to access that object to get the author name. Call this column ```author_name`` in your CSV file. </span>\n",
    "\n",
    "**Note: Some posts will not have data for some of these columns. That is fine! You can make these fields blank in the CSV, then.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_capture = [ 'created_utc', \n",
    "                     'is_crosspostable', 'is_self', 'is_video', 'locked', 'media_only', 'over_18',\n",
    "                     'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', \n",
    "                     'title', 'permalink', \n",
    "                     'total_awards_received', 'downs','gilded','num_comments', 'num_crossposts', 'num_reports', \n",
    "                     'ups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code here to save out the data\n",
    "\n",
    "# TODO: REPLACE\n",
    "\n",
    "data_to_save = []\n",
    "for submission in submissions:\n",
    "    s = vars(submission)\n",
    "    sub_dict = {f : s[f] for f in fields_to_capture}\n",
    "    sub_dict['author_name'] = submission.author.name if submission.author else \"\"\n",
    "    data_to_save.append(sub_dict)\n",
    "\n",
    "pd.DataFrame(data_to_save).drop_duplicates(\"permalink\").to_csv(\"part2_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Grading for saving data\n",
    "\n",
    "Submit your saved data from the reddit API in a file named ```part1_data.csv```. For grading for Part 1.3, we will check that:\n",
    "- We can read in the saved file using ```pandas.read_csv()```\n",
    "- The resulting file has data from three subreddits, approximately 1000 from each (give or take what the API decides to give you, which is out of your control).\n",
    "- The resulting file has all the necessary columns, i.e. those listed in ```fields_to_capture```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Analyzing an existing dataset\n",
    "\n",
    "For this section of the assignment, the entire class will use ```part2_data.csv```.  We will ask you to analyze these data in a variety of ways. Parts of this will be submitted in your written report, and other parts will be evaluated automatically.\n",
    "\n",
    "**Part 2 data consists of the top 1000 (ish) posts from 24 different subreddits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_data = pd.read_csv(\"474_s22_assignments/assignment1/part2_data.csv\")\n",
    "part2_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 - Quick Descriptive Analyses\n",
    "\n",
    "Answers to each of the questions below should be provided in your written report. Additionally, we expect code to be written below that shows how you obtained answers to each of these questions. We will spot-check several of these. **Note, each of these can be answered using *only pandas*. \n",
    "\n",
    "\n",
    "### Univariate descriptive analyses\n",
    "- **2.1.1** What are the names (```subreddit_name_prefixed```) of the 25 different subreddits that are in ```part2_data.csv```?\n",
    "- **2.1.2** How many reddit authors (```author_name```) have a post in more than one unique subreddit in ```part2_data.csv``` (e.g. they have a top post in both ```r/news``` and ```r/hockey```)?\n",
    "- **2.1.3** What is the mean number of upvotes (```ups```) for posts in ```r/Jokes```?\n",
    "- **2.1.4** What is the variance of the number of upvotes in ```r/news```?\n",
    "- **2.1.5** What is the standard deviation of the number of upvotes received across the entire dataset? \n",
    "- **2.1.6** (No code for this) Mathematically, what is the relationship between the standard deviation of the number of upvotes and the variance of upvotes?\n",
    "- **2.1.7** Which subreddit had the third highest median number of upvotes?\n",
    "\n",
    "### Conditional probability\n",
    "- **2.1.8** What is the conditional probability of an author having a top post in ```r/news```, given that they have a top post in ```r/worldnews```?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.1 here\n",
    "part2_data.subreddit_name_prefixed.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.2 here\n",
    "authors = part2_data.groupby(\"author_name\").subreddit_name_prefixed.nunique().reset_index()\n",
    "authors[authors.subreddit_name_prefixed > 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.3 here\n",
    "part2_data[part2_data.subreddit_name_prefixed == 'r/Jokes'].ups.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.4 here\n",
    "part2_data[part2_data.subreddit_name_prefixed == 'r/news'].ups.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.5 here\n",
    "part2_data.ups.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "# Put your code for 2.1.7 here\n",
    "part2_data.groupby(\"subreddit_name_prefixed\").ups.median().sort_values(ascending=False).index[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your code for 2.1.8 here\n",
    "\n",
    "\n",
    "authors_posting_in_worldnews = part2_data[part2_data.subreddit_name_prefixed == \n",
    "                                          'r/worldnews'].author_name.unique()\n",
    "\n",
    "authors_posting_in_news = part2_data[part2_data.subreddit_name_prefixed == \n",
    "                                          'r/news'].author_name.unique()\n",
    "\n",
    "### Using the \"tree\" approach\n",
    "n_posted_in_both = len(set(authors_posting_in_news) & \n",
    "                       set(authors_posting_in_worldnews))\n",
    "n_posted_in_worldnews = len(authors_posting_in_worldnews)\n",
    "print(n_posted_in_both/n_posted_in_worldnews)\n",
    "\n",
    "### Using law of conditional probability\n",
    "n_total_authors = part2_data.author_name.nunique()\n",
    "p_posted_in_both = n_posted_in_both/n_total_authors\n",
    "p_posted_only_worldnews = n_posted_in_worldnews/n_total_authors\n",
    "p_posted_in_both/p_posted_only_worldnews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 - Plotting and the like\n",
    "Where we have asked you to create a plot below, make sure to provide the resulting plot in your written report.  \n",
    "\n",
    "**You are free to use whatever plotting software you wish! Although I personally think [seaborn](https://seaborn.pydata.org/), [plotly](https://plotly.com/python), or [altair](https://altair-viz.github.io/) will make these the easiest, the lecture notes also have examples using matplotlib.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 - Histograms\n",
    "\n",
    "Plot a histogram for the distribution of upvotes for each subreddit separately (*hint: you will want to use \"faceting\" to make this easy on yourself*). **All plot titles and axis labels should be legible in the PDF you submit**.\n",
    "\n",
    "- **2.2.1** - Submit your histogram image in your assignment\n",
    "- **2.2.2** - Based on your histogram, which subreddit would you say is the *least* popular? (Note, there is more than one reasonable answer here. We are looking mostly for how you justify your response using the histogram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "fig = px.histogram(part2_data, x='ups', facet_col='subreddit_name_prefixed', \n",
    "                   facet_col_wrap=4,\n",
    "            height=1000, width=1000)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting and using the empirical CDF\n",
    "\n",
    "The *[empirical cumulative distribution function (eCDF)](https://en.wikipedia.org/wiki/Empirical_distribution_function)* is an empirical estimator for the CDF of a random variable. Below we have plotted for you (using ```plotly```) the eCDFs of the distribution of upvotes for three different subreddits. Using the plots below, answer the following questions:\n",
    "\n",
    "**Note, you can use your mouse to scroll over the information in the plot, that will make answering these questions much easier!**\n",
    "\n",
    "- **2.2.3** - **Approximately (within 1-2 percentage points)** what percent of top posts for each of the three subreddits plotted below have less than 100,000 upvotes? (Give answers for each subreddit)\n",
    "- **2.2.4** - **Approximately (within 1-2 percentage points)** what is the probability that a post on each of the three subreddits plotted below has more than 70,000 upvotes? (Give answers for each subreddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.ecdf(part2_data[part2_data.subreddit_name_prefixed.isin([\"r/news\",\"r/worldnews\",\"r/science\"])], \n",
    "              x=\"ups\",\n",
    "              facet_col='subreddit_name_prefixed',\n",
    "             height=400,width=800)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Trends \n",
    "\n",
    "To answer this question we are going to plot the average upvotes and number of top posts of a subreddit in our dataset per each year.\n",
    "\n",
    "First, add a ```year``` column to the data, that represents the year in which the post was sent. You likely want to use the [pandas documentation on dates and times](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_data = part2_data.assign(year= pd.to_datetime(part2_data.created_utc,unit='s').apply(lambda x: x.year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check on this column, answer the following question:\n",
    "- **2.2.5** - How many posts in the dataset were sent in 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE\n",
    "year_data = part2_data.groupby(\"year\").size().reset_index()\n",
    "int(year_data[year_data.year == 2010][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're going to plot the **yearly trend** of average upvotes for each subreddit.\n",
    "\n",
    "**Hint: We will assume that the average upvotes for a given subreddit in a given year is zero when there are no top posts for that subreddit in that year.** To accurately reflect this, you will have to make sure to account for this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: you do not have to use this function, but it may be useful for you\n",
    "from itertools import product\n",
    "\n",
    "## TODO: REPLACE\n",
    "plot_data = (part2_data.groupby(['year','subreddit_name_prefixed'])\n",
    "             .ups\n",
    "             .mean()\n",
    "             .reset_index()\n",
    "            )\n",
    "\n",
    "# create the zeros dataframe\n",
    "merge_zeros = pd.DataFrame(product(part2_data.year.unique(), \n",
    "                     part2_data.subreddit_name_prefixed.unique()),\n",
    "                     columns =['year','subreddit_name_prefixed']\n",
    "                )\n",
    "merge_zeros['ups_with_zeros'] = 0\n",
    "\n",
    "# merge\n",
    "plot_data =merge_zeros.merge(plot_data, how=\"left\")\n",
    "null_rows = ~pd.isnull(plot_data.ups)\n",
    "plot_data.loc[null_rows, \"ups_with_zeros\"] = plot_data[null_rows].ups\n",
    "plot_data.drop(columns=\"ups\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, please do the following:\n",
    "\n",
    "- **2.2.6** - In your report, provide a table (a screenshot of a pandas dataframe is fine) that shows the average number of upvotes for r/memes each year from 2015 to 2020. The table should be sorted by year (i.e. 2015, then 2016, etc.). Note again, if a year does not have data, there should be zeros in this table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Replace\n",
    "\n",
    "plot_data[(plot_data.subreddit_name_prefixed == \"r/memes\") &\n",
    "          (plot_data.year < 2021) & \n",
    "          (plot_data.year > 2014)].sort_values(\"year\", ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2.2.7** - Plot a line graph of the temporal trend of mean upvotes from 2016-2020 for the following subreddits: r/Jokes, r/food,r/conspiracy, and r/news . You can plot them individually, or use the faceting approach from above. Write your code for this in the cell below; copy the resulting plot to your PDF report. **Hint: Doing part 2.2.8 will be easiest if you make sure that the plot for each subreddit has its own y-axis!**. \n",
    "- **2.2.8** - Using what you have plotted, make an argument for which of the four subreddits is the most \"up and coming\" - i.e. the one that seems to be getting more popular over time. NOTE: There is more than one reasonable answer here. We are looking for how you justify your answer using the (plotted) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: REPLACE \n",
    "fig = px.line(plot_data[(plot_data.subreddit_name_prefixed.isin(['r/Jokes','r/food','r/conspiracy','r/news'])) &\n",
    "                        (plot_data.year > 2015) & \n",
    "                        (plot_data.year < 2022) \n",
    "                        ].sort_values(\"year\"), \n",
    "              x=\"year\",\n",
    "                y=\"ups_with_zeros\",\n",
    "                facet_col=\"subreddit_name_prefixed\",\n",
    "                facet_col_wrap=2,\n",
    "                markers = True,         \n",
    "                height=600,\n",
    "                width=700)\n",
    "fig.for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
    "fig.update_xaxes(matches=None)\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 - Data Cleaning and some final regression-oriented data exploration\n",
    "\n",
    "With the above analysis, we've learned some things about what predicts upvotes:\n",
    "- Which subreddit the post is in is seems to matter quite a bit for the number of upvotes\n",
    "- Time: there are temporal trends, although separate for each subreddit, that seem to be predictive\n",
    "\n",
    "As we gear up to create our linear regression model to try to predict the number of upvotes for posts, we are going to turn to two last steps:\n",
    "1. Data cleaning - we're going to take a look at some bivariate statistics, which are going to reveal some columns in our data that are not useful.  We'll then remove them.\n",
    "2. Looking at univariate relationships with our outcome - we are going to plot relationships between a few of the remaining interesting continuous variables and our outcome of interest (upvotes)\n",
    "\n",
    "\n",
    "### Cleaning our data\n",
    "\n",
    "Below, we list the columns of our dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part2_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the continuous variables. Those are:\n",
    "- ```total_awards_received```\n",
    "- ```downs```\n",
    "- ```gilded```\n",
    "- ```num_comments```\n",
    "- ```num_crossposts```\n",
    "- ```num_reports```\n",
    "- ```created_utc```\n",
    "- ```subreddit_subscribers```\n",
    "\n",
    "- **2.3.1**-  There are two continuous variables that are very clearly not going to be useful for our analysis. Identify them, and explain why they are not useful (**note: you do NOT need to know why these variables take on the values they do in our data. You just need to know why we don't want to use them!**)\n",
    "\n",
    "Let's now look at our (supposedly) binary categorical variables:\n",
    "- ```is_crosspostable```\n",
    "- ```is_self```\n",
    "- ```media_only```\n",
    "- ```is_video```\n",
    "- ```locked```\n",
    "- ```over_18```\n",
    "\n",
    "- **2.3.2**-  There are two (supposedly) binary variables that are very clearly not going to be useful for our analysis. Identify them, and explain why they are not useful (**note: you do NOT need to know why these variables take on the values they do in our data. You just need to know why we don't want to use them!**)\n",
    "\n",
    "Finally, let's look at our remaining variables, which are categorical. One of these, ```title``` (the post's title), is potentially a *very* useful feature... but we haven't yet learned how to use it. So, for now, we're not going to.  The other categorical features are:\n",
    "- ```subreddit_id```\n",
    "- ```subreddit_name_prefixed```\n",
    "- ```permalink```\n",
    "\n",
    "- **2.3.3** -  Explain why we it is not useful to use *both* ```subreddit_id``` and ```subreddit_name_prefixed``` in any predictive analysis of per-post upvotes.\n",
    "- **2.3.4** - Explain why it is not useful to use ```permalink``` in any predictive analysis of per-post upvotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "# downs, num_reports, \n",
    "\n",
    "# is_crosspostable, media_only, \n",
    "# subreddit_id\n",
    "# title\n",
    "# permalink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate relationships with the outcome\n",
    "\n",
    "- **2.3.5** - Plot the relationship between ```num_comments``` and upvotes as a scatterplot with log-scaled axes, with the posts from different subreddits as different color points. Paste this plot into your PDF writeup\n",
    "\n",
    "- **2.3.6** - Describe, briefly (a sentence) the relationship between ```num_comments``` and upvotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: REPLACE\n",
    "px.scatter(part2_data, x ='ups', y ='num_comments',color='subreddit_name_prefixed',\n",
    "           log_x=True, \n",
    "           log_y=True,\n",
    "           height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the [Pearson correlation](https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html#pandas.Series.corr) between ```ups``` and all other continuous variables (minus those you identified as not interesting in 2.3.1).\n",
    "\n",
    "- **2.3.7** - Which of these has the strongest positive correlation with ```ups```?\n",
    "- **2.3.8** - Which of these has the weakest positive correlation with ```ups```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TODO: REPLACE\n",
    "for var in [\n",
    "'total_awards_received',\n",
    "'downs',\n",
    "'gilded',\n",
    "'num_comments',\n",
    "'num_crossposts',\n",
    "'created_utc',\n",
    "'subreddit_subscribers'\n",
    "]:\n",
    "    print(var)\n",
    "    print(part2_data['ups'].corr(part2_data[var]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Linear Regression\n",
    "\n",
    "OK! We've got a decent handle on our data, and we're ready to do some learning. \n",
    "\n",
    "We're going to use a linear regression model to predict the number of upvotes.\n",
    "\n",
    "## Part 3.1 - Regression to predict ```ups```\n",
    "\n",
    "You will need to write code that does the following:\n",
    "\n",
    "1. Recreates (if you did not already store it in your dataset) a variable for the year a post was sent in. Now, create a column ```year```, which. Then, subsets your data to only posts from 2015-2021 (inclusive).\n",
    "\n",
    "2. Creates a feature matrix, ```X```, that contains features for the following variables:\n",
    "- ```total_awards_received```\n",
    "- ```gilded```\n",
    "- ```num_comments```\n",
    "- ```num_crossposts```\n",
    "- ```year```\n",
    "- ```is_self```\n",
    "- ```is_video```\n",
    "- ```locked```\n",
    "- ```over_18```\n",
    "- ```subreddit_name_prefixed```\n",
    "\n",
    "3. Creates an outcome variable, ```y```, that is **the logarithm of** ```ups +1```.\n",
    "4. Splits the data into train and test (80% training, 20% testing) using the relevant ```sklearn``` function. **We have written this line of code for you below, please do not change the random state!**\n",
    "5. Trains a linear regression model on the training data\n",
    "6. Evaluates the model you have trained on the test set, using ```RMSE``` as an error metric. **You should calculate this error using ONLY ```pandas``` and/or ```numpy```, not ```sklearn```.**\n",
    "7. Prints the error\n",
    "\n",
    "A few useful hints:\n",
    "- You cannot use ```subreddit_name_prefixed``` as is, you have to transform it somehow. We have suggested a tool to do so below (the ```OneHotEncoder```)\n",
    "- You also need to transform any boolean variables to 0/1 encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets just reload the data in to make sure we're all starting fresh!\n",
    "part3_data = pd.read_csv(\"474_s22_assignments/assignment1/part2_data.csv\")\n",
    "part3_data['year'] = pd.to_datetime(part3_data.created_utc,unit='s').apply(lambda x: x.year)\n",
    "part3_data = part3_data[(part3_data.year <2022) & (part3_data.year > 2014)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## NOTE: Typically we would not rescale a time variable, but it's fine for this assignment.\n",
    "CONTINUOUS_VARS = [\"total_awards_received\", \"gilded\", \"num_comments\", \"num_crossposts\",\"created_utc\"]\n",
    "BINARY_VARS = [\"is_self\", \"is_video\", \"locked\", \"over_18\"]\n",
    "\n",
    "\n",
    "for var in BINARY_VARS:\n",
    "    # Write your code here to make sure the boolean variables are formatted as integers, as is required by sklearn\n",
    "    \n",
    "    # TODO: REPLACE\n",
    "    part3_data[var] = part3_data[var].astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def onehot_encode_var(data, varname):\n",
    "    # This function should take in a variable name in part3_data and return a onehot encoded matrix for that variable\n",
    "    \n",
    "    # Here's a starting point!\n",
    "    encoder = OneHotEncoder(drop = \"first\")\n",
    "\n",
    "    # TODO: Replace\n",
    "    encoder = encoder.fit(data[[varname]])\n",
    "    onehot_encoded_variable = encoder.transform(data[[varname]]).todense()\n",
    "    \n",
    "    # return the onehot encoded variable\n",
    "    return onehot_encoded_variable, encoder.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, now we're going to write our code to run the model!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, rescale the continuous variables\n",
    "continuous_rescaled_X = StandardScaler().fit_transform(part3_data[CONTINUOUS_VARS].values)\n",
    "\n",
    "# Now, we can use our function above to get the onehotencoding for the \n",
    "onehot_encoded_subreddit, onehot_encoded_subreddit_categories = onehot_encode_var(part3_data, \n",
    "                                                                                  \"subreddit_name_prefixed\")\n",
    "\n",
    "# Now you can combine all of your features into a single feature matrix. Call it X\n",
    "X = np.hstack([np.matrix(continuous_rescaled_X ),\n",
    "               np.matrix(part3_data[BINARY_VARS].values),\n",
    "              onehot_encoded_subreddit]\n",
    "             )\n",
    "\n",
    "# And create your outcome variable, call it y\n",
    "\n",
    "y = np.log(part3_data.ups + 1)\n",
    "\n",
    "# Don't change this line!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n",
    "# fit a linear regression model, with an intercept\n",
    "lmodel = LinearRegression()\n",
    "lmodel.fit(X_train,y_train)\n",
    "\n",
    "# Compute RMSE\n",
    "\n",
    "RMSE = np.sqrt(((lmodel.predict(X_test)-y_test)**2).sum()/len(y_test))\n",
    "exp_RMSE = np.sqrt(((np.exp(lmodel.predict(X_test))-np.exp(y_test))**2).sum()/len(y_test))\n",
    "print(f\"RMSE: {RMSE}\")\n",
    "print(f\"RMSE original scale: {exp_RMSE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions to check understanding\n",
    "\n",
    "- **3.1.1** - Report your error on the test data, in RMSE. State what this metric means for the expected error in terms of the number of upvotes (not log upvotes!) you should expect to be off on any given prediction\n",
    "\n",
    "Also, a few questions to target your understanding of how we set up the model:\n",
    "- **3.1.2** - What did the whole one-hot encoding thing on ```subreddit_name_prefixed``` actually do? \n",
    "- **3.1.3** - What does the argument ```drop = \"first\"``` do for us when we are doing that to ```subreddit_name_prefixed```?\n",
    "- **3.1.3** - Why did we need to add one to the outcome variable before using ```log```?\n",
    "- **3.1.4** - What does the ```StandardScaler``` do? Why do we want to do that?\n",
    "\n",
    "\n",
    "OK. Having looked at our RMSE, we should now realize that we have to be careful about assuming that this one statistic is actually a good estimate of how far we're going to be off on any prediction, selected at random. To see this, let's do the following:\n",
    "- **3.1.5** - Provide a scatterplot that compares the true values in ```y_test``` to the absolute value of the difference between ```y_test``` and your predictions. **The axes should be on the original scale** (i.e. not the log scale you're predicting on.\n",
    "- **3.1.6** - What does this plot suggest about how well your model fits the data as the true number of upvotes changes? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(zip(np.exp(lmodel.predict(X_test)),\n",
    "                   np.exp(y_test.values)),\n",
    "              columns=['predicted','true'])\n",
    "res['diff'] = np.abs(res['predicted']-res['true'])\n",
    "px.scatter(res, x='true',y='diff', trendline='ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One final thing we are going to play with in 3.1. Logging the dependent variable is useful for a few reasons we have or will discuss in class (depending on when you're reading this). But it's also sometimes useful to log *independent* variables as well. Below, redo the same analysis as above, but after logging the non-temporal continuous variables (i.e. all the continuous variables except ```created_utc```). Use these as predictors instead of the original continuous variables. **Note: Perform the logging before you rescale the variables. Also, you should add 1 as we did for the dependent variable above**.\n",
    "\n",
    "- **3.1.7** - What is the new RMSE with the logged independent variables?\n",
    "- **3.1.8** - How did this compare to the old RMSE? Why do you think that is? Hint: It may help to re-plot the same figure as you did in 3.1.5, but with the new model, in order to answer this question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the continuous variables\n",
    "\n",
    "for var in CONTINUOUS_VARS:\n",
    "    part3_data[var] = np.log(part3_data[var]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, now we're going to write our code to run the model!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# First, rescale the continuous variables\n",
    "continuous_rescaled_X = StandardScaler().fit_transform(part3_data[CONTINUOUS_VARS].values)\n",
    "\n",
    "# Now you can combine all of your features into a single feature matrix. Call it X\n",
    "## TODO: Replace\n",
    "X = np.hstack([np.matrix(continuous_rescaled_X ),\n",
    "               np.matrix(part3_data[BINARY_VARS].values),\n",
    "              onehot_encoded_subreddit]\n",
    "             )\n",
    "\n",
    "# And create your outcome variable, call it y\n",
    "\n",
    "y = np.log(part3_data.ups + 1)\n",
    "\n",
    "# Don't change this line!\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=1)\n",
    "\n",
    "# fit a linear regression model, with an intercept\n",
    "\n",
    "lmodel = LinearRegression()\n",
    "lmodel.fit(X_train,y_train)\n",
    "\n",
    "# Compute RMSE\n",
    "\n",
    "RMSE = np.sqrt(((lmodel.predict(X_test)-y_test)**2).sum()/len(y_test))\n",
    "print(f\"RMSE: {RMSE}\")\n",
    "\n",
    "res = pd.DataFrame(zip(np.exp(lmodel.predict(X_test)),\n",
    "                   np.exp(y_test.values)),\n",
    "              columns=['predicted','true'])\n",
    "res['diff'] = np.abs(res['predicted']-res['true'])\n",
    "px.scatter(res, x='true',y='diff', trendline='ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.2 - Exploration of regression coefficients\n",
    "\n",
    "Now, let's look at the effects of our variables for this last model (with the logarithms of the independent variables). Carefully re-combine your features with their labels (*hint, ```encoder.categories_``` will be your friend, and remember, we dropped the first category!*)\n",
    "\n",
    "- **3.2.1** - What is the strongest positive predictor of upvotes? How many more log(upvotes+1) does a one standard deviation increase in the feature correspond to?\n",
    "- **3.2.2** - What is the strongest negative predictor of upvotes? How many fewer log(upvotes+1) does a one standard deviation increase in the feature correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code for 3.2 here\n",
    "\n",
    "# TODO: REPLACE\n",
    "categories = CONTINUOUS_VARS+ BINARY_VARS + list(onehot_encoded_subreddit_categories)\n",
    "pd.DataFrame(zip(lmodel.coef_.tolist(), categories),\n",
    "             columns=['val','label']).sort_values(\"val\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3.3 - 574 Only - Attempting to Improve Your Predictions \n",
    "\n",
    "In class, we talked about a few things we might do to improve our model's predictions. These include adding interaction terms, including different functional forms of a feature, using a different model, etc. Here, we ask that you implement at least two of these, and then re-evaluate your model. We'll ask some of the teams with some of the more interesting/effective ideas here to come present their solutions to the class!\n",
    "\n",
    "- **3.3.1** - Describe at least two changes you made -- at least one to the feature set, and at least one different model -- to try to improve prediction.  Explain *why* you think that these changes make sense, given the Exploratory analyeses above, or any other exploratory analysis you choose to do.\n",
    "- **3.3.2** - Did your RMSE improve? Why or why not, do you think?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code for 3.3.1 here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

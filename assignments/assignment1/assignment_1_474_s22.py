# -*- coding: utf-8 -*-
"""assignment_1_474_s22.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YJ2VqElyF5LFaEPmfz2i-2YZuBNf5ODq

# Programming Assignment 1 - Understanding upvotes on top reddit posts

After completing this project, you will be able to do the following:

- Collect and save reddit data using the reddit API (through the ```praw``` library)
- Be able to conduct descriptive analyses, via manipulation of data stored in a pandas dataframe, and via the creation and exploration of graphs, of the number of upvotes of reddit comments
- Be able to conduct a linear regression to help understand the factors associated with a top post having many upvotes on reddit
- **574 Only**: Be able to implement additional feature sets and/or a new model and describe why those decisions were made and what their effects were on performance

# Resources you can use to complete this assignment (a COMPLETE list)

**NOTE: You ARE allowed to use Google to find things that fit this list (i.e. it is often easy to google something like "plotly draw line graph" to find the right part of the plotly documentation).**

- Anything linked to in this article
- Anything linked to from the course web page
- Any materials from another online course taught at a university (**if you use this, you MUST provide a link to the exact document used**)
- Anything posted by Kenny, Navid, or Yincheng on Piazza

# Setup

- For this assignment, you will need to install the PRAW library for scraping reddit data.

# Grading

There are three parts to the grading:

1. **Written Report (60 points)**: You will submit a PDF report that provides answers to questions here, and that contains plots we request.  These same questions are also posted in the assignment PDF, for convenience. **Again, the questions in the Assignment PDF and here are the same (for the written report), we just put them in both places for convenience.**.

2. **Saved file from Part 1.3 (10 points)**: See below for details. 

3. **Coding spot checks (30 points)** - We will select 6 problems to spot check. This means that we will check to make sure that your code is written in a reasonable way and that it obtains the desired results when we run your code. For example, your code should not be written in a way that makes it exceedingly slow, e.g. by using for loops where a vectorized approach would be applicable. We will *not* tell you which problems we are spot checking.


As such, you will submit, one member of your group will subit as a zip file on UBLearns, a ```.zip``` file that contains 3 things:
- Your completed jupyter notebook.
- Your written report, answering all questions asked here (and copied in the assignment PDF)
- Your saved file from Part 1.3 below
"""

# This is a comment in the code. All comments in python are preceeded by a pound sign
# Comments can be plain English, because the computer ignores them when running the code.

# This should be all the imports you need for this project.

# The line of code below this comment imports code written by other people in the form of the 
# praw library

import praw

import numpy as np
import pandas as pd
import plotly.express as px

pip install praw

"""# Part 1: Data collection

## Step 1: Creating a reddit account

If you don't have one already, the first thing you'll need to do is go to [reddit](http://www.reddit.com/) and create a reddit account.

## Step 2: Creating a reddit app

Now, we're going to create a reddit app. Make sure you're signed in to your reddit account, and then go to the [app page](https://ssl.reddit.com/prefs/apps/).  From here, click on the "create an app" button. <b>Make sure that you've selected the "script" option in the checkbox</b>, and then fill in a name and description. For the two URL fields, it doesn't really matter what you put. Your input should look something like mine.

<img width="1000px" src="./reddit_tut_0.png" style="max-width:95%;border:3px solid black;"></img>
    
Cool! Now hit "create app" button.

## Step 3: Scraping some reddit data

### 3a - setting up your API credentials
We're now going to pull down some real, live reddit posts and take a look at them! To do so, we're going to need some information from our app.  Reddit uses this information to keep track of who is accessing their data, and in what ways they're accessing it.  In other words, reddit wants to make sure it knows who might be doing bad things with their data or to their platform, so they can shut those apps down. We'll return to that point in a bit.  But, for now, you'll need to edit the code below to enter in your ```client_id```, your ```client_secret```, and your ```username```  in order for you to be able to collect data.  ***Note - PLEASE TRY TO REMEMBER TO REMOVE YOUR SECRET INFORMATION BEFORE SUBMITTING THE ASSIGNMENT***. You can find the first two on the app creation page, where the red boxes are in the image below:

<img width="1000px" src="reddit_tut_1.png" style="max-width:95%;border:3px solid black;"></img>

Enter your information between the appropriate quotation marks in the (python) code below, replacing the phrase ```ENTER_YOUR_XXX_HERE```.

 ***Make sure to push ```SHIFT+ENTER``` after you've changed the code!***
"""

your_client_id = 'wDxIubyU9BesVrXGc89HQg'
your_client_secret = 'CqGtFqsKchhbI0zfEyYIfCtDcB58KQ'
your_username = "annkonna"

"""### 3b - Selecting subreddits

Ok, we're almost all set up to scrape! to do so, we're going to use a python library called [praw](https://praw.readthedocs.io/).  Praw is a relatively powerful tool, allowing you to do a bunch of cool things with the reddit API.  For this assignment, you're just going to do the basics:

Specifically, **<span style="color: red;">create a variable called ```subreddits``` in the code box below. The variable should point to a ```list``` data structure that has the names of 3 subreddits you want to pull data from.</span>**
"""

subreddits = ['3Dprinting','funny','memes']

"""### 3c - Setting up our authentication mechanism for our application

The last step before we start crawling is to set ourselves up to make authenticated calls to the reddit API. **<span style="color: red;">Use the PRAW library to create an instance of the class ```praw.Reddit``` that you can use to scrape the reddit API.</span>**

"""

# Code for 3c should go here
reddit = praw.Reddit(
    client_id=your_client_id,
    client_secret=your_client_secret,
    user_agent="annkonna",
    check_for_async=False
)

"""### 3d - Finally, some scraping!

OK! Now we can finally pull some data down from the reddit API!

<span style="color: red;">Use the ```praw``` library to pull down the **top 1000 posts of all time from EACH of the 3 subreddits you selected.** Note: You may not get all 1000, due to oddities with the reddit API. However, your code should specify that it *wants* up to 1000 posts.</span>
"""

# Code for 3d should go here
'''subreddit1 = reddit.subreddit('3Dprinting')
subreddit2 = reddit.subreddit('funny')
subreddit3 = reddit.subreddit('memes')
subreddit_top_1000_posts = []
for submission in subreddit1.top(limit=1000):
    subreddit_top_1000_posts.append([submission.url, submission.title, submission.score, submission.id])
for submission in subreddit2.top(limit=1000):
    subreddit_top_1000_posts.append([submission.url, submission.title, submission.score, submission.id])
for submission in subreddit3.top(limit=1000):
    subreddit_top_1000_posts.append([submission.url, submission.title, submission.score, submission.id])
subreddit_top_1000_posts'''
subreddit_top_1000_posts = []
for subredditname in subreddits:
  for submission in reddit.subreddit(subredditname).top("all",limit=1000):
    subreddit_top_1000_posts.append(submission)
# subreddit_top_1000_posts
print(len(subreddit_top_1000_posts))

"""## Answering some questions about your data and the API

### 1.1 Understanding APIs
***Note, Part 1 questions can be answered by carefully reading the [documentation of the PRAW library carefully](https://praw.readthedocs.io/en/v3.6.2/pages/getting_started.html) and/or the [reddit API documentation](https://github.com/reddit-archive/reddit/wiki/API#rules).***

- **1.1.1** How many API calls were required to collect the submissions?
- **1.1.2** Why did we set the submission limit at 1000?
- **1.1.3** How long, in minutes, would it take you to collect 1000 posts from 25 different subreddits? What about from 500 different subreddits? *Hint: You'll have to consider how many API requests you are allowed to make in a given time period.*

### 1.2 Thinking about your sample

You collected (approximately) the top 1000 submissions from 3 different subreddits. 

- **1.2.1** Do you think these posts are representative of **all** the posts on that subreddit? (Yes or no, only) 
- **1.2.2** Why or why not? That is, if you think so, why do you think there's not much sampling bias here? If not, what do you think might be different about these top posts than other posts?

## Saving out your data

Finally, we're going to save your data out and submit it. For this part, [this section of the API documentation may be useful](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html#determine-available-attributes-of-an-object). Similarly, note that saving CSVs is sometimes easiest by first converting your data into a ```pandas``` dataframe, and then just calling ```.to_csv()```.

<span style="color: red;">You will save the data you have collected out to a CSV file. This CSV file should be called ```part1_data.csv```. The file should contain a column for each of the fields listed in the ```fields_to_capture``` list below. Additionally, you should save out the **author's name** (hint, the author attribute of the ```praw.Submission``` data structure is a ```praw.Redditor``` object. You will need to access that object to get the author name. Call this column ```author_name`` in your CSV file. </span>

**Note: Some posts will not have data for some of these columns. That is fine! You can make these fields blank in the CSV, then.**
"""

# Don't change this!
fields_to_capture = [ 'created_utc', 
                     'is_crosspostable', 'is_self', 'is_video', 'locked', 'media_only', 'over_18',
                     'subreddit_id', 'subreddit_name_prefixed', 'subreddit_subscribers', 
                     'title', 'permalink', 
                     'total_awards_received', 'downs','gilded','num_comments', 'num_crossposts', 'num_reports', 
                     'ups']

# Write the code here to save out the data
import pprint
import pandas as pd
'''subreddit1 = reddit.subreddit('3Dprinting')
s = subreddit1.top(limit=2)'''
subreddit_info_list = list()
print(len(subreddit_top_1000_posts))
print(type(subreddit_top_1000_posts[0]))
for item in subreddit_top_1000_posts:
  info_dict = dict()
  if item.author!=None:
    info_dict["author_name"]=item.author.name  #added author name to the dataframe
  for field in fields_to_capture:
    info_dict[field] = vars(item)[field]
    #pprint.pprint(vars(item)[field])
  subreddit_info_list.append(info_dict)

  # print(subreddit_info_list)

csv_data = pd.DataFrame(subreddit_info_list).to_csv('part1_data.csv')
df=pd.read_csv('part1_data.csv')
print(len(df))

"""### 1.3 Grading for saving data

Submit your saved data from the reddit API in a file named ```part1_data.csv```. For grading for Part 1.3, we will check that:
- We can read in the saved file using ```pandas.read_csv()```
- The resulting file has data from three subreddits, approximately 1000 from each (give or take what the API decides to give you, which is out of your control).
- The resulting file has all the necessary columns, i.e. those listed in ```fields_to_capture```.

# Part 2 - Analyzing an existing dataset

For this section of the assignment, the entire class will use ```part2_data.csv```.  We will ask you to analyze these data in a variety of ways. Parts of this will be submitted in your written report, and other parts will be evaluated automatically.

**Part 2 data consists of the top 1000 (ish) posts from 24 different subreddits**.
"""

from google.colab import files
uploaded = files.upload()

import io
df2 = pd.read_csv(io.BytesIO(uploaded['part2_data.csv']))

part2_data = pd.read_csv("./part2_data.csv")
part2_data.head()
len(part2_data)

"""## Part 2.1 - Quick Descriptive Analyses

Answers to each of the questions below should be provided in your written report. Additionally, we expect code to be written below that shows how you obtained answers to each of these questions. We will spot-check several of these. **Note, each of these can be answered using *only pandas*. 


### Univariate descriptive analyses
- **2.1.1** What are the names (```subreddit_name_prefixed```) of the 25 different subreddits that are in ```part2_data.csv```?
- **2.1.2** How many reddit authors (```author_name```) have a post in more than one unique subreddit in ```part2_data.csv``` (e.g. they have a top post in both ```r/news``` and ```r/hockey```)?
- **2.1.3** What is the mean number of upvotes (```ups```) for posts in ```r/Jokes```?
- **2.1.4** What is the variance of the number of upvotes in ```r/news```?
- **2.1.5** What is the standard deviation of the number of upvotes received across the entire dataset? 
- **2.1.6** (No code for this) Mathematically, what is the relationship between the standard deviation of the number of upvotes and the variance of upvotes?
- **2.1.7** Which subreddit had the third highest median number of upvotes?

### Conditional probability
- **2.1.8** What is the conditional probability of an author having a top post in ```r/news```, given that they have a top post in ```r/worldnews```?

"""

# Put your code for 2.1.1 here
'''column = ["subreddit_name_prefixed"]
df = pd.read_csv("part2_data.csv", usecols=column, index_col=False)
subreddit_names = df.drop_duplicates()
print(subreddit_names.to_string(index=False))'''
part2_data.subreddit_name_prefixed.unique().tolist()   #found a much faster way imo, feel free to change if old one was faster

# Put your code for 2.1.2 here
df = pd.read_csv("part2_data.csv")
pd.set_option('display.max_rows', None)
df1 = df.groupby(['author_name']).subreddit_name_prefixed.value_counts()
df2 = df1[df1.groupby(level=0).transform('nunique').gt(1)]
print(len(df2))    #question asks to print length

# Put your code for 2.1.3 here
df = pd.read_csv("part2_data.csv")
loc = df[df['subreddit_name_prefixed'] == 'r/Jokes']
print(loc['ups'].mean())

# Put your code for 2.1.4 here
df = pd.read_csv("part2_data.csv")
loc = df.loc[df['subreddit_name_prefixed'] == 'r/news']
print(loc['ups'].var())

# Put your code for 2.1.5 here
df = pd.read_csv("part2_data.csv")
print(df['ups'].std())

# Put your code for 2.1.6 here

"""Variance of upvotes is the square of the standard deviation of the number of upvotes."""

# Put your code for 2.1.7 here
df = pd.read_csv("part2_data.csv")
'''median_subreddits = []
for sub in subreddit_names.values.tolist():
  loc = df.loc[df['subreddit_name_prefixed'] == sub[0]]
  median_subreddits.append(loc['ups'].median())
median_subreddits.sort()
print(median_subreddits[-3])'''

a=df.groupby(['subreddit_name_prefixed'])['ups'].median().tolist()        #changed to this
a.sort(reverse=True)
print(a[2])

# Put your code for 2.1.8 here
author_column = ["author_name"]
author_df = pd.read_csv("part2_data.csv", usecols=author_column, index_col=False)
author_names = author_df.drop_duplicates()
author_names_len = len(author_df.drop_duplicates())
author_news_loc = df[df['subreddit_name_prefixed'] == 'r/news']
author_news = author_news_loc["author_name"].drop_duplicates()
author_news_len = len(author_news)
author_worldnews_loc = df.loc[df['subreddit_name_prefixed'] == 'r/worldnews']
author_worldnews = author_worldnews_loc["author_name"].drop_duplicates()
author_worldnews_len = len(author_worldnews)
both_news_count = 0
for worldnews in author_worldnews.values.tolist():
    if worldnews in author_news.values.tolist():
        both_news_count += 1
prob_news = author_news_len / author_names_len
prob_worldnews = author_worldnews_len / author_names_len
prob_bothnews = both_news_count / author_names_len
prob_news_given_worldnews = prob_bothnews / prob_worldnews
print(prob_news_given_worldnews)

"""## Part 2.2 - Plotting and the like
Where we have asked you to create a plot below, make sure to provide the resulting plot in your written report.  

**You are free to use whatever plotting software you wish! Although I personally think [seaborn](https://seaborn.pydata.org/), [plotly](https://plotly.com/python), or [altair](https://altair-viz.github.io/) will make these the easiest, the lecture notes also have examples using matplotlib.**

### Part 2.1 - Histograms

Plot a histogram for the distribution of upvotes for each subreddit separately (*hint: you will want to use "faceting" to make this easy on yourself*). **All plot titles and axis labels should be legible in the PDF you submit**.

- **2.2.1** - Submit your histogram image in your assignment
- **2.2.2** - Based on your histogram, which subreddit would you say is the *least* popular? (Note, there is more than one reasonable answer here. We are looking mostly for how you justify your response using the histogram)
"""

# Code for 2.2.1 here
import matplotlib.pyplot as plt
df = pd.read_csv("part2_data.csv")
fig = px.histogram(df, x="subreddit_name_prefixed", y="ups")
fig.show()

"""### Ploting and using the empirical CDF

The *[empirical cumulative distribution function (eCDF)](https://en.wikipedia.org/wiki/Empirical_distribution_function)* is an empirical estimator for the CDF of a random variable. Below we have plotted for you (using ```plotly```) the eCDFs of the distribution of upvotes for three different subreddits. Using the plots below, answer the following questions:

**Note, you can use your mouse to scroll over the information in the plot, that will make answering these questions much easier!**

- **2.2.3** - **Approximately (within 1-2 percentage points)** what percent of top posts for each of the three subreddits plotted below have less than 100,000 upvotes? (Give answers for each subreddit)
- **2.2.4** - **Approximately (within 1-2 percentage points)** what is the probability that a post on each of the three subreddits plotted below has more than 70,000 upvotes? (Give answers for each subreddit)
"""

import plotly.express as px

fig = px.ecdf(part2_data[part2_data.subreddit_name_prefixed.isin(["r/news","r/worldnews","r/science"])], 
              x="ups",
              facet_col='subreddit_name_prefixed',
             height=400,width=800)
fig.for_each_annotation(lambda a: a.update(text=a.text.split("=")[-1]))
fig.update_xaxes(matches=None)
fig.show()

"""### Temporal Trends 

To answer this question we are going to plot the average upvotes and number of top posts of a subreddit in our dataset per each year.

First, add a ```year``` column to the data, that represents the year in which the post was sent. You likely want to use the [pandas documentation on dates and times](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html) for this.
"""

# Add the year column
import datetime
df = pd.read_csv("part2_data.csv")
df['year'] = pd.to_datetime(df['created_utc'], unit='s').dt.year
df.to_csv('part2_data.csv', index=False)
df.columns

"""As a check on this column, answer the following question:
- **2.2.5** - How many posts in the dataset were sent in 2010?
"""

# Code for 2.2.5 here
year_column = ["year"]
year_df = pd.read_csv("part2_data.csv", usecols=year_column, index_col=False)
year_loc = df.loc[df['year'] == 2010]
print(year_loc['num_crossposts'].sum())

"""Now, we're going to plot the **yearly trend** of average upvotes for each subreddit.

**Hint: We will assume that the average upvotes for a given subreddit in a given year is zero when there are no top posts for that subreddit in that year.** To accurately reflect this, you will have to make sure to account for this case.

"""

# Hint: you do not have to use this function, but it may be useful for you
from itertools import product

# create the zeros dataframe
merge_zeros = pd.DataFrame(product(part2_data.year.unique(), 
                     part2_data.subreddit_name_prefixed.unique()),
                     columns =['year','subreddit_name_prefixed']
                )
merge_zeros['ups_with_zeros'] = 0

# merge with the non-zero data ... you write this code ...
merge_non_zeros = pd.DataFrame(product(part2_data.year.unique(), 
                     part2_data.subreddit_name_prefixed.unique()),
                     columns =['year','subreddit_name_prefixed']
                    )
subreddit_unique_list = part2_data.subreddit_name_prefixed.unique().tolist()
year_list = part2_data.year.unique().tolist()
merge_non_zeros['ups_with_zeros'] = 0
for year in year_list:
    for subreddit_name in subreddit_unique_list:
        loc = df[(df['subreddit_name_prefixed'] == subreddit_name) & (df['year'] == year)]
        merge_non_zeros.loc[(merge_non_zeros['subreddit_name_prefixed'] == subreddit_name) & (merge_non_zeros['year'] == year), 'ups_with_zeros'] = loc['ups'].mean()
yearly_trend = merge_non_zeros.combine_first(merge_zeros)
yearly_trend

"""As a check, please do the following:

- **2.2.6** - In your report, provide a table (a screenshot of a pandas dataframe is fine) that shows the average number of upvotes for r/memes each year from 2015 to 2020. The table should be sorted by year (i.e. 2015, then 2016, etc.). Note again, if a year does not have data, there should be zeros in this table!
"""

# Code for 2.2.6 here
merge_avg_upvotes = pd.DataFrame(product(part2_data.year.unique(), 
                     part2_data.subreddit_name_prefixed.unique()),
                     columns =['year','subreddit_name_prefixed']
                )
merge_zero_upvotes = pd.DataFrame(product(part2_data.year.unique(), 
                     part2_data.subreddit_name_prefixed.unique()),
                     columns =['year','subreddit_name_prefixed']
                )
merge_zero_upvotes['average_ups'] = 0
merge_avg_upvotes['average_ups'] = 0
merge_avg_upvotes.drop(merge_avg_upvotes.index[(merge_avg_upvotes['year'] < 2015) | (merge_avg_upvotes['year'] > 2020)], inplace=True)
merge_avg_upvotes.drop(merge_avg_upvotes.index[merge_avg_upvotes['subreddit_name_prefixed'] != 'r/memes'], inplace=True)
merge_zero_upvotes.drop(merge_zero_upvotes.index[(merge_zero_upvotes['year'] < 2015) | (merge_zero_upvotes['year'] > 2020)], inplace=True)
merge_zero_upvotes.drop(merge_zero_upvotes.index[merge_zero_upvotes['subreddit_name_prefixed'] != 'r/memes'], inplace=True)
year_list_15_20 = [year for year in year_list if (year >= 2015 and year <= 2020)]
for year in year_list_15_20:
    for subreddit_name in subreddit_unique_list:
        loc = df[(df['subreddit_name_prefixed'] == 'r/memes') & (df['year'] == year)]
        merge_avg_upvotes.loc[(merge_avg_upvotes['subreddit_name_prefixed'] == 'r/memes') & (merge_avg_upvotes['year'] == year), 'average_ups'] = loc['ups'].mean()
memes_trend = merge_avg_upvotes.combine_first(merge_zero_upvotes)
memes_trend.sort_values(by='year')

"""- **2.2.7** - Plot a line graph of the temporal trend of mean upvotes from 2016-2020 for the following subreddits: r/Jokes, r/food,r/conspiracy, and r/news . You can plot them individually, or use the faceting approach from above. Write your code for this in the cell below; copy the resulting plot to your PDF report. **Hint: Doing part 2.2.8 will be easiest if you make sure that the plot for each subreddit has its own y-axis!**. 
- **2.2.8** - Using what you have plotted, make an argument for which of the four subreddits is the most "up and coming" - i.e. the one that seems to be getting more popular over time. NOTE: There is more than one reasonable answer here. We are looking for how you justify your answer using the (plotted) data.
"""

# Code for 2.2.7 here

"""## Part 2.3 - Data Cleaning and some final regression-oriented data exploration

With the above analysis, we've learned some things about what predicts upvotes:
- Which subreddit the post is in is seems to matter quite a bit for the number of upvotes
- Time: there are temporal trends, although separate for each subreddit, that seem to be predictive

As we gear up to create our linear regression model to try to predict the number of upvotes for posts, we are going to turn to two last steps:
1. Data cleaning - we're going to take a look at some bivariate statistics, which are going to reveal some columns in our data that are not useful.  We'll then remove them.
2. Looking at univariate relationships with our outcome - we are going to plot relationships between a few of the remaining interesting continuous variables and our outcome of interest (upvotes)


### Cleaning our data

Below, we list the columns of our dataset...
"""

part2_data.columns

"""Let's start by looking at the continuous variables. Those are:
- ```total_awards_received```
- ```downs```
- ```gilded```
- ```num_comments```
- ```num_crossposts```
- ```num_reports```
- ```created_utc```
- ```subreddit_subscribers```

- **2.3.1**-  There are two continuous variables that are very clearly not going to be useful for our analysis. Identify them, and explain why they are not useful (**note: you do NOT need to know why these variables take on the values they do in our data. You just need to know why we don't want to use them!**)

Let's now look at our (supposedly) binary categorical variables:
- ```is_crosspostable```
- ```is_self```
- ```media_only```
- ```is_video```
- ```locked```
- ```over_18```

- **2.3.2**-  There are two (supposedly) binary variables that are very clearly not going to be useful for our analysis. Identify them, and explain why they are not useful (**note: you do NOT need to know why these variables take on the values they do in our data. You just need to know why we don't want to use them!**)

Finally, let's look at our remaining variables, which are categorical. One of these, ```title``` (the post's title), is potentially a *very* useful feature... but we haven't yet learned how to use it. So, for now, we're not going to.  The other categorical features are:
- ```subreddit_id```
- ```subreddit_name_prefixed```
- ```permalink```

- **2.3.3** -  Explain why we it is not useful to use *both* ```subreddit_id``` and ```subreddit_name_prefixed``` in any predictive analysis of per-post upvotes.
- **2.3.4** - Explain why it is not useful to use ```permalink``` in any predictive analysis of per-post upvotes.

## Univariate relationships with the outcome

- **2.3.5** - Plot the relationship between ```num_comments``` and upvotes as a scatterplot with log-scaled axes, with the posts from different subreddits as different color points. Paste this plot into your PDF writeup

- **2.3.6** - Describe, briefly (a sentence) the relationship between ```num_comments``` and upvotes.
"""

# Code for 2.3.5 here
subreddit_names = df['subreddit_name_prefixed']
upvotes = df['ups']
number_of_comments = df['num_comments']

plt.scatter(x=upvotes, y=number_of_comments, c=subreddit_names.astype('category').cat.codes)
plt.show()

"""Compute the [Pearson correlation](https://pandas.pydata.org/docs/reference/api/pandas.Series.corr.html#pandas.Series.corr) between ```ups``` and all other continuous variables (minus those you identified as not interesting in 2.3.1).

- **2.3.7** - Which of these has the strongest positive correlation with ```ups```?
- **2.3.8** - Which of these has the weakest positive correlation with ```ups```?
"""

# Code for 2.3.7-8 here

#part2_data.corr(method='pearson')

og_part2_data_columns = part2_data.corrwith(part2_data['ups'], method='pearson')
# dropping the columns that are not interesting
part2_data_columns = og_part2_data_columns.drop(["is_crosspostable", "media_only"])
# 2.3.7 : Strongest positive correlation
print(part2_data_columns.nlargest(n=1).where(part2_data_columns > 0))
# 2.3.8 : Weakest positive correlation 
print(part2_data_columns.nsmallest(n=1).mask(part2_data_columns > 0))

"""# Part 3 - Linear Regression

OK! We've got a decent handle on our data, and we're ready to do some learning. 

We're going to use a linear regression model to predict the number of upvotes.

## Part 3.1 - Regression to predict ```ups```

You will need to write code that does the following:

1. Recreates (if you did not already store it in your dataset) a variable for the year a post was sent in. Now, create a column ```year```, which. Then, subsets your data to only posts from 2015-2021 (inclusive).

2. Creates a feature matrix, ```X```, that contains features for the following variables:
- ```total_awards_received```
- ```gilded```
- ```num_comments```
- ```num_crossposts```
- ```year```
- ```is_self```
- ```is_video```
- ```locked```
- ```over_18```
- ```subreddit_name_prefixed```

3. Creates an outcome variable, ```y```, that is **the logarithm of** ```ups +1```.
4. Splits the data into train and test (80% training, 20% testing) using the relevant ```sklearn``` function. **We have written this line of code for you below, please do not change the random state!**
5. Trains a linear regression model on the training data
6. Evaluates the model you have trained on the test set, using ```RMSE``` as an error metric. **You should calculate this error using ONLY ```pandas``` and/or ```numpy```, not ```sklearn```.**
7. Prints the error

A few useful hints:
- You cannot use ```subreddit_name_prefixed``` as is, you have to transform it somehow. We have suggested a tool to do so below (the ```OneHotEncoder```)
- You also need to transform any boolean variables to 0/1 encodings

"""

# Lets just reload the data in to make sure we're all starting fresh!
import numpy as np
import pandas as pd
import datetime

part3_data = pd.read_csv("part2_data.csv")
part3_data['year'] = pd.to_datetime(part3_data['created_utc'], unit='s').dt.year

part3_data = part3_data.loc[part3_data['year'].isin([x for x in range(2015, 2022)])]
# part3_data.year.unique().tolist()
# part3_data.head()

## NOTE: Typically we would not rescale a time variable, but it's fine for this assignment.
CONTINUOUS_VARS = ["total_awards_received", "gilded", "num_comments", "num_crossposts","created_utc"]
BINARY_VARS = ["is_self", "is_video", "locked", "over_18"]


for var in BINARY_VARS:
    # Write your code here to make sure the boolean variables are formatted as integers, as is required by sklearn
    part3_data[var] = part3_data[var].astype(int)
    
# part3_data.head()

from sklearn.preprocessing import OneHotEncoder

def onehot_encode_var(data, varname):
    # This function should take in a variable name in part3_data and return a onehot encoded matrix for that variable
    
    # Here's a starting point!
    encoder = OneHotEncoder(drop='first')
   
    # Use the encoder
    onehot_encoded_variable = pd.DataFrame(encoder.fit_transform(data[[varname]]).toarray())
    onehot_encoded_variable.columns = encoder.get_feature_names_out() # Give names to one hot encoded columns. Otherwise it will be numeric
    
    # return the onehot encoded variable
    return onehot_encoded_variable, encoder.categories_

# OK, now we're going to write our code to run the model!
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

feature_names = ['total_awards_received', 'gilded', 'num_comments', 'num_crossposts', 
                 'year', 'is_self', 'is_video', 'locked', 'over_18', 'subreddit_name_prefixed']

# First, rescale the continuous variables
continuous_rescaled_X = StandardScaler().fit_transform(part3_data[CONTINUOUS_VARS].values)

X = part3_data[feature_names]

# Now, we can use our function above to get the onehotencoding for the subreddits ... go ahead!

encoded_col, _ = onehot_encode_var(X, 'subreddit_name_prefixed')

# Now you can combine all of your features into a single feature matrix. Call it X

X.drop(columns='subreddit_name_prefixed', axis=1, inplace=True)

encoded_col.reset_index(drop=True, inplace=True)
X.reset_index(drop=True, inplace=True)
X = pd.concat([encoded_col, X], axis = 1)


# And create your outcome variable, call it y

y = np.log2(part3_data['ups'] + 1) # Since some ups are 0, we need +1 to avoid np.log returing divide by zero error.
                                   # Try removing +1 to see it yourself
print(y.shape)
#print(X.columns)
#print(X.head())
#print(y.head())

# Don't change this line!
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2, 
                                                    random_state=1)

print(y_test.shape)

# fit a linear regression model, with an intercept
reg = LinearRegression().fit(X_train, y_train)

# Make predictions using the testing set
y_pred = reg.predict(X_test)

# Compute RMSE
print("Mean squared error: %.2f" % mean_squared_error(y_test, y_pred))

"""### Questions to check understanding

- **3.1.1** - Report your error on the test data, in RMSE. State what this metric means for the expected error in terms of the number of upvotes (not log upvotes!) you should expect to be off on any given prediction

Also, a few questions to target your understanding of how we set up the model:
- **3.1.2** - What did the whole one-hot encoding thing on ```subreddit_name_prefixed``` actually do? 
- **3.1.3** - What does the argument ```drop = "first"``` do for us when we are doing that to ```subreddit_name_prefixed```?
- **3.1.3** - Why did we need to add one to the outcome variable before using ```log```?
- **3.1.4** - What does the ```StandardScaler``` do? Why do we want to do that?


OK. Having looked at our RMSE, we should now realize that we have to be careful about assuming that this one statistic is actually a good estimate of how far we're going to be off on any prediction, selected at random. To see this, let's do the following:
- **3.1.5** - Provide a scatterplot that compares the true values in ```y_test``` to the absolute value of the difference between ```y_test``` and your predictions. **The axes should be on the original scale** (i.e. not the log scale you're predicting on.
- **3.1.6** - What does this plot suggest about how well your model fits the data as the true number of upvotes changes? 

"""

# Code for 3.1.5 here

# y_diff = abs(y_test - y_pred)

#print(y_diff.head)

#print(y_test[0], y_pred[0])
#print(2 ** y_test[0])
#y_test = [2**i - 1 for i in y_test]
print(y_test[0])
y_test = y_test.rpow(2) - 1
print(y_test[0])

#print(part3_data['ups'][0])
np.log2(part3_data['ups'][0] + 1)



"""One final thing we are going to play with in 3.1. Logging the dependent variable is useful for a few reasons we have or will discuss in class (depending on when you're reading this). But it's also sometimes useful to log *independent* variables as well. Below, redo the same analysis as above, but after logging the non-temporal continuous variables (i.e. all the continuous variables except ```created_utc```). Use these as predictors instead of the original continuous variables. **Note: Perform the logging before you rescale the variables. Also, you should add 1 as we did for the dependent variable above**.

- **3.1.7** - What is the new RMSE with the logged independent variables?
- **3.1.8** - How did this compare to the old RMSE? Why do you think that is? Hint: It may help to re-plot the same figure as you did in 3.1.5, but with the new model, in order to answer this question.

"""

# Code for 3.1.7 here

"""## Part 3.2 - Exploration of regression coefficients

Now, let's look at the effects of our variables for this last model (with the logarithms of the independent variables). Carefully re-combine your features with their labels (*hint, ```encoder.categories_``` will be your friend, and remember, we dropped the first category!*)

- **3.2.1** - What is the strongest positive predictor of upvotes? How many more log(upvotes+1) does a one standard deviation increase in the feature correspond to?
- **3.2.2** - What is the strongest negative predictor of upvotes? How many fewer log(upvotes+1) does a one standard deviation increase in the feature correspond to?
"""

# Add your code for 3.2 here

"""# Part 3.3 - 574 Only - Attempting to Improve Your Predictions 

In class, we talked about a few things we might do to improve our model's predictions. These include adding interaction terms, including different functional forms of a feature, using a different model, etc. Here, we ask that you implement at least two of these, and then re-evaluate your model. We'll ask some of the teams with some of the more interesting/effective ideas here to come present their solutions to the class!

- **3.3.1** - Describe at least two changes you made -- at least one to the feature set, and at least one different model -- to try to improve prediction.  Explain *why* you think that these changes make sense, given the Exploratory analyeses above, or any other exploratory analysis you choose to do.
- **3.3.2** - By how much did your RMSE improve? Which change that you made improved it the most? How do you know?
"""

# Add your code for 3.3.1 here

